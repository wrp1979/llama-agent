# llama-agent

A coding agent that runs entirely inside [llama.cpp](https://github.com/ggml-org/llama.cpp): single binary, zero dependencies, native performance.

<img width="1500" height="960" alt="image" src="https://github.com/user-attachments/assets/7f917819-50ab-447f-9504-6406b2670ad5" />

## What is it?

`llama-agent` builds on llama.cpp's inference engine and adds an agentic tool-use loop on top. The result:

- **Single binary**: no Python, no Node.js, just download and run
- **Native speed**: tool calls in-process, no HTTP overhead
- **100% local**: offline, no API costs, your code stays on your machine

## Quick Start

```bash
# Build
cmake -B build
cmake --build build --target llama-agent

# Run (downloads model automatically)
./build/bin/llama-agent -hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q5_K_M

# Or with a local model
./build/bin/llama-agent -m model.gguf
```

## Installation

To use `llama-agent` from any directory, add it to your PATH:

```bash
# Run from the llama.cpp directory after building
echo "export PATH=\"\$PATH:$(pwd)/build/bin\"" >> ~/.zshrc

# Open a new terminal, then run from anywhere
cd /path/to/your/project
llama-agent -hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q5_K_M
```

## Recommended Model

| Model | Command |
|-------|---------|
| Nemotron-3-Nano 30B | `-hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q5_K_M` |

## Available Tools

| Tool | Description |
|------|-------------|
| `bash` | Execute shell commands |
| `read` | Read file contents with line numbers |
| `write` | Create or overwrite files |
| `edit` | Search and replace in files |
| `glob` | Find files matching a pattern |
| `task` | Spawn a subagent for complex tasks |

## Subagents

Subagents are specialized child agents that handle complex tasks independently, keeping the main conversation context clean and efficient.

### Why Subagents?

Without subagents, every file read and search pollutes your main context:

```
Main context after exploring codebase:
├── glob **/*.cpp → 50 files (800 tokens)
├── read src/main.cpp → full file (1,500 tokens)
├── read src/utils.cpp → full file (2,200 tokens)
├── grep "TODO" → 100 matches (1,200 tokens)
└── Total: ~5,700 tokens consumed for ONE exploration
```

With subagents, only the summary enters main context:

```
Main context:
└── task(explore) → "Found 3 TODO items in src/main.cpp:42,87,156" (50 tokens)

Subagent context (discarded after):
├── All the detailed exploration (~5,700 tokens)
└── Summarized to parent
```

### Subagent Types

| Type | Purpose | Tools Available |
|------|---------|-----------------|
| `explore` | Search and understand code (read-only) | `glob`, `read`, `bash` (read-only commands only) |
| `bash` | Execute shell commands | `bash` |
| `plan` | Design implementation approaches | `glob`, `read`, `bash` |
| `general` | General-purpose tasks | All tools |

### How It Works

```
┌─────────────────┐
│   Main Agent    │  "Find where errors are handled"
└────────┬────────┘
         │ task(explore, "find error handling")
         ▼
┌─────────────────┐
│    Subagent     │  Does detailed exploration:
│    (explore)    │  - glob **/*.cpp
│                 │  - read 5 files
│                 │  - grep patterns
└────────┬────────┘
         │ Returns summary only
         ▼
┌─────────────────┐
│   Main Agent    │  Receives: "Errors handled in src/error.cpp:45
│                 │  via ErrorHandler class..."
└─────────────────┘
```

### Memory Efficiency

Subagents share the model - no additional VRAM is used:

| Resource | Main Agent | Subagent | Total |
|----------|------------|----------|-------|
| Model weights | ✓ | Shared | 1x |
| KV cache | ✓ | Shared via slots | 1x |
| Context window | Own | Own (discarded after) | Efficient |

### Parallel Execution

Multiple subagents can run in the background simultaneously:

```
> Run tests and check for lint errors at the same time

[task-a1b2] ┌── ⚡ run-tests (bash)
[task-c3d4] ┌── ⚡ check-lint (bash)
[task-a1b2] │   ├─› bash npm test (2.1s)
[task-c3d4] │   ├─› bash npm run lint (1.8s)
[task-c3d4] │   └── done (1.8s)
[task-a1b2] │   └── done (2.1s)
```

### KV Cache Prefix Sharing

Subagent prompts share a common prefix with the main agent, enabling automatic KV cache reuse:

```
Main agent prompt:    "You are llama-agent... [base] + [main agent instructions]"
Subagent prompt:      "You are llama-agent... [base] + # Subagent Mode: explore..."
                       ↑─────── shared prefix ──────↑
                       Cached tokens reused, not re-processed
```

This reduces subagent startup latency and saves compute.

## Usage Examples

```
> Find all TODO comments in src/

[Tool: bash] grep -r "TODO" src/
Found 5 TODO comments...

> Read the main.cpp file

[Tool: read] main.cpp
   1| #include <iostream>
   2| int main() {
   ...

> Fix the bug on line 42

[Tool: edit] main.cpp
Replaced "old code" with "fixed code"
```

## Commands

| Command | Description |
|---------|-------------|
| `/exit` | Exit the agent |
| `/clear` | Clear conversation history |
| `/tools` | List available tools |
| `/skills` | List available skills |
| `/agents` | List discovered AGENTS.md files |

## Skills

Skills are reusable prompt modules that extend the agent's capabilities. They follow the [agentskills.io](https://agentskills.io) specification.

### Creating a Skill

Skills are directories containing a `SKILL.md` file with YAML frontmatter:

```bash
mkdir -p ~/.llama-agent/skills/code-review
cat > ~/.llama-agent/skills/code-review/SKILL.md << 'EOF'
---
name: code-review
description: Review code for bugs, security issues, and improvements. Use when asked to review code or a PR.
---

# Code Review Instructions

When reviewing code:
1. Run `git diff` to see changes
2. Read modified files for context
3. Check for bugs, security issues, style problems
4. Provide specific feedback with file:line references
EOF
```

### Skill Structure

```
skill-name/
├── SKILL.md          # Required - YAML frontmatter + instructions
├── scripts/          # Optional - executable scripts
├── references/       # Optional - additional documentation
└── assets/           # Optional - templates, data files
```

### SKILL.md Format

```yaml
---
name: skill-name          # Required: 1-64 chars, lowercase+numbers+hyphens
description: What and when # Required: 1-1024 chars, triggers activation
license: MIT              # Optional
compatibility: python3    # Optional: environment requirements
metadata:                 # Optional: custom key-value pairs
  author: someone
---

Markdown instructions for the agent...
```

### Search Paths

Skills are discovered from (in priority order):

1. `./.llama-agent/skills/` - Project-local skills
2. `~/.llama-agent/skills/` - User-global skills
3. Custom paths via `--skills-path`

### CLI Options

| Flag | Description |
|------|-------------|
| `--no-skills` | Disable skill discovery |
| `--skills-path PATH` | Add custom skills directory |

### How Skills Work

1. **Discovery**: At startup, the agent scans skill directories and loads metadata (name/description)
2. **Activation**: When your request matches a skill's description, the agent reads the full `SKILL.md`
3. **Execution**: The agent follows the skill's instructions, optionally running scripts from `scripts/`

This "progressive disclosure" keeps context lean - only activated skills consume tokens.

## AGENTS.md Support

The agent automatically discovers and loads [AGENTS.md](https://agents.md) files, which provide project-specific guidance for AI coding assistants.

### How It Works

1. **Discovery**: At startup, the agent searches from the working directory up to the git root for `AGENTS.md` files, plus a global file
2. **Precedence**: Files closer to the working directory take precedence (useful for monorepos). Global file has lowest precedence.
3. **Injection**: Content is loaded into the system prompt, giving the agent project-specific context

### Search Locations (in precedence order)

1. `./AGENTS.md` - Current working directory (highest precedence)
2. `../AGENTS.md`, `../../AGENTS.md`, ... - Parent directories up to git root
3. `~/.llama-agent/AGENTS.md` - Global user preferences (lowest precedence)

### Creating an AGENTS.md File

Create an `AGENTS.md` file in your repository root:

```markdown
# Project Guidelines

## Build & Test
- Build: `cmake -B build && cmake --build build`
- Test: `ctest --test-dir build`

## Code Style
- Use 4-space indentation
- Follow Google C++ style guide

## PR Guidelines
- Include tests for new features
- Update documentation
```

### Monorepo Support

In monorepos, you can have nested `AGENTS.md` files:

```
repo/
├── AGENTS.md           # General project guidance
├── packages/
│   ├── frontend/
│   │   └── AGENTS.md   # Frontend-specific guidance (takes precedence)
│   └── backend/
│       └── AGENTS.md   # Backend-specific guidance
```

When working in `packages/frontend/`, both `AGENTS.md` files are loaded, with the frontend one taking precedence.

### CLI Options

| Flag | Description |
|------|-------------|
| `--no-agents-md` | Disable AGENTS.md discovery |

## MCP Server Support

The agent supports [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers, allowing you to extend its capabilities with external tools.

### Configuration

Create an `mcp.json` file in your working directory or at `~/.llama-agent/mcp.json`:

```json
{
  "servers": {
    "gradio": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "https://tongyi-mai-z-image-turbo.hf.space/gradio_api/mcp/",
        "--transport",
        "streamable-http"
      ],
      "timeout": 120000
    }
  }
}
```

### Config Options

| Field | Description | Default |
|-------|-------------|---------|
| `command` | Executable to run (required) | - |
| `args` | Command line arguments | `[]` |
| `env` | Environment variables | `{}` |
| `timeout` | Tool call timeout in ms | `60000` |
| `enabled` | Enable/disable the server | `true` |

Environment variables in config values can use `${VAR_NAME}` syntax for substitution.

### Transport

Only **stdio** transport is supported natively. The agent spawns the server process and communicates via stdin/stdout using JSON-RPC 2.0.

For HTTP-based MCP servers (like Gradio endpoints), use a bridge such as `mcp-remote` as shown in the example above.

### Tool Naming

MCP tools are registered with qualified names: `mcp__<server>__<tool>`. For example, a `read_file` tool from a server named `filesystem` becomes `mcp__filesystem__read_file`.

Use `/tools` to see all available tools including MCP tools.

## Permission System

The agent asks for confirmation before:
- Running shell commands
- Writing or editing files
- Accessing files outside the working directory

When prompted: `y` (yes), `n` (no), `a` (always allow), `d` (deny always)

### Safety Features

- **Sensitive file blocking**: Automatically blocks access to `.env`, `*.key`, `*.pem`, credentials files
- **External directory warnings**: Prompts before accessing files outside the project
- **Dangerous command detection**: Warns for `rm -rf`, `sudo`, `curl|bash`, etc.
- **Doom-loop detection**: Detects and blocks repeated identical tool calls

### CLI Options

| Flag | Description |
|------|-------------|
| `--yolo` | Skip all permission prompts (dangerous!) |
| `--max-iterations N` | Max agent iterations (default: 50, max: 1000) |
| `--no-skills` | Disable skill discovery |
| `--skills-path PATH` | Add custom skills search path |
| `--no-agents-md` | Disable AGENTS.md discovery |

### YOLO Mode

Skip all permission prompts:

```bash
./build/bin/llama-agent -m model.gguf --yolo
```

> [!CAUTION]
> **YOLO mode is extremely dangerous.** The agent will execute any command without confirmation, including destructive operations like `rm -rf`. This is especially risky with smaller models that have weaker instruction-following and may hallucinate unsafe commands. Only use this flag if you fully trust the model and understand the risks.

## License

MIT - see [LICENSE](LICENSE)
